{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YT2TR4wp2_Ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616452b6-b1c7-43a4-d4f1-41f7f4760c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Bertie Woosters Feature Selection Algorithm.\n",
            "Type in the name of the file to test: CS170_Small_Data__47.txt\n",
            "\n",
            "Type the number of the algorithm you want to run.\n",
            "\n",
            "1) Forward Selection\n",
            "2) Backward Elimination\n",
            "1\n",
            "\n",
            "This dataset has 6 features (not including the class attribute), with 500 instances.\n",
            "\n",
            "Running nearest neighbor with all 6 features, using \"leaving-one-out\" evaluation, I get an accuracy of 81.8%\n",
            "\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) [1] accuracy is 73.8%\n",
            "Using feature(s) [2] accuracy is 71.8%\n",
            "Using feature(s) [3] accuracy is 70.8%\n",
            "Using feature(s) [4] accuracy is 83.4%\n",
            "Using feature(s) [5] accuracy is 71.6%\n",
            "Using feature(s) [6] accuracy is 73.4%\n",
            "\n",
            "Feature set [4] was best, accuracy is 83.4%\n",
            "\n",
            "Using feature(s) [4, 1] accuracy is 96.0%\n",
            "Using feature(s) [4, 2] accuracy is 83.4%\n",
            "Using feature(s) [4, 3] accuracy is 81.8%\n",
            "Using feature(s) [4, 5] accuracy is 86.4%\n",
            "Using feature(s) [4, 6] accuracy is 84.4%\n",
            "\n",
            "Feature set [4, 1] was best, accuracy is 96.0%\n",
            "\n",
            "Using feature(s) [4, 1, 2] accuracy is 91.6%\n",
            "Using feature(s) [4, 1, 3] accuracy is 90.4%\n",
            "Using feature(s) [4, 1, 5] accuracy is 94.2%\n",
            "Using feature(s) [4, 1, 6] accuracy is 93.4%\n",
            "\n",
            "Feature set [4, 1, 5] was best, accuracy is 94.2%\n",
            "\n",
            "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
            "\n",
            "Using feature(s) [4, 1, 5, 2] accuracy is 88.6%\n",
            "Using feature(s) [4, 1, 5, 3] accuracy is 88.8%\n",
            "Using feature(s) [4, 1, 5, 6] accuracy is 90.2%\n",
            "\n",
            "Feature set [4, 1, 5, 6] was best, accuracy is 90.2%\n",
            "\n",
            "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
            "\n",
            "Using feature(s) [4, 1, 5, 6, 2] accuracy is 86.4%\n",
            "Using feature(s) [4, 1, 5, 6, 3] accuracy is 84.6%\n",
            "\n",
            "Feature set [4, 1, 5, 6, 2] was best, accuracy is 86.4%\n",
            "\n",
            "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
            "\n",
            "Using feature(s) [4, 1, 5, 6, 2, 3] accuracy is 81.8%\n",
            "\n",
            "Feature set [4, 1, 5, 6, 2, 3] was best, accuracy is 81.8%\n",
            "\n",
            "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
            "\n",
            "Finished search!! The best feature subset is [4, 1], which has an accuracy of 96.0%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"Loads the dataset from a text file.\"\"\"\n",
        "    data = np.loadtxt(filename)\n",
        "    return data\n",
        "\n",
        "def euclidean_distance(instance1, instance2):\n",
        "    \"\"\"Computes the Euclidean distance between two instances.\"\"\"\n",
        "    return np.sqrt(np.sum((instance1 - instance2) ** 2))\n",
        "\n",
        "def leave_one_out_cross_validation(data, feature_subset):\n",
        "    \"\"\"Evaluates nearest neighbor accuracy using Leave-One-Out Cross Validation (LOOCV).\"\"\"\n",
        "    num_correct = 0\n",
        "    num_instances = data.shape[0]\n",
        "\n",
        "\n",
        "    for i in range(num_instances):\n",
        "        test_instance = data[i, feature_subset]\n",
        "        test_label = data[i, 0]\n",
        "\n",
        "        min_distance = float('inf')\n",
        "        predicted_label = None\n",
        "\n",
        "        # Computing Nearest Neighbor\n",
        "        for j in range(num_instances):\n",
        "            if i == j:\n",
        "                continue\n",
        "            train_instance = data[j, feature_subset]\n",
        "            train_label = data[j, 0]\n",
        "\n",
        "            distance = euclidean_distance(test_instance, train_instance)\n",
        "\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                predicted_label = train_label\n",
        "\n",
        "        if predicted_label == test_label:\n",
        "            num_correct += 1\n",
        "\n",
        "    return (num_correct / num_instances) * 100\n",
        "\n",
        "def forward_selection(data):\n",
        "    \"\"\"Performs feature selection using Forward Selection.\"\"\"\n",
        "    num_features = data.shape[1] - 1\n",
        "    current_features = []\n",
        "    best_overall_features = []\n",
        "    best_overall_accuracy = 0\n",
        "\n",
        "    search_path = []  # Stores the search path followed\n",
        "    accuracy_path = []  # Stores accuracy at each step\n",
        "\n",
        "    print(\"\\nBeginning search.\\n\")\n",
        "\n",
        "    for i in range(num_features):\n",
        "        best_feature = None\n",
        "        best_accuracy = 0\n",
        "\n",
        "        for feature in range(1, num_features + 1):\n",
        "            if feature not in current_features:\n",
        "                temp_features = current_features + [feature]\n",
        "                accuracy = leave_one_out_cross_validation(data, temp_features)\n",
        "                print(f\"Using feature(s) {temp_features} accuracy is {accuracy:.1f}%\")\n",
        "\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_feature = feature\n",
        "\n",
        "        if best_feature:\n",
        "            current_features.append(best_feature)\n",
        "            search_path.append(list(current_features))\n",
        "            accuracy_path.append(best_accuracy)\n",
        "            print(f\"\\nFeature set {current_features} was best, accuracy is {best_accuracy:.1f}%\\n\")\n",
        "\n",
        "            if best_accuracy < best_overall_accuracy:\n",
        "                print(\"(Warning, Accuracy has decreased! Continuing search in case of local maxima)\\n\")\n",
        "\n",
        "            if best_accuracy > best_overall_accuracy:\n",
        "                best_overall_accuracy = best_accuracy\n",
        "                best_overall_features = list(current_features)\n",
        "\n",
        "    print(f\"Finished search!! The best feature subset is {best_overall_features}, which has an accuracy of {best_overall_accuracy:.1f}%\\n\")\n",
        "    return best_overall_features, best_overall_accuracy, search_path, accuracy_path\n",
        "\n",
        "def backward_elimination(data):\n",
        "    \"\"\"Performs feature selection using Backward Elimination.\"\"\"\n",
        "    num_features = data.shape[1] - 1\n",
        "    current_features = list(range(1, num_features + 1))  # Start with all features\n",
        "    best_overall_features = list(current_features)\n",
        "    best_overall_accuracy = leave_one_out_cross_validation(data, current_features)\n",
        "\n",
        "    search_path = [list(current_features)]  # Stores the search path followed\n",
        "    accuracy_path = [best_overall_accuracy]  # Stores accuracy at each step\n",
        "\n",
        "    print(\"\\nBeginning search.\\n\")\n",
        "\n",
        "    for i in range(num_features - 1):  # We remove features one by one\n",
        "        worst_feature = None\n",
        "        best_accuracy = 0\n",
        "\n",
        "        for feature in current_features:\n",
        "            temp_features = [f for f in current_features if f != feature]  # Remove one feature\n",
        "            accuracy = leave_one_out_cross_validation(data, temp_features)\n",
        "            print(f\"Using feature(s) {temp_features} accuracy is {accuracy:.1f}%\")\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                worst_feature = feature  # We will remove this feature\n",
        "\n",
        "        if worst_feature:\n",
        "            current_features.remove(worst_feature)\n",
        "            search_path.append(list(current_features))\n",
        "            accuracy_path.append(best_accuracy)\n",
        "            print(f\"\\nFeature set {current_features} was best, accuracy is {best_accuracy:.1f}%\\n\")\n",
        "\n",
        "            if best_accuracy < best_overall_accuracy:\n",
        "                print(\"(Warning, Accuracy has decreased! Continuing search in case of local maxima)\\n\")\n",
        "\n",
        "            if best_accuracy > best_overall_accuracy:\n",
        "                best_overall_accuracy = best_accuracy\n",
        "                best_overall_features = list(current_features)\n",
        "\n",
        "    print(f\"Finished search!! The best feature subset is {best_overall_features}, which has an accuracy of {best_overall_accuracy:.1f}%\\n\")\n",
        "    return best_overall_features, best_overall_accuracy, search_path, accuracy_path\n",
        "\n",
        "# Main function to execute the program\n",
        "def main():\n",
        "    print(\"Welcome to Bertie Woosters Feature Selection Algorithm.\")\n",
        "    filename = input(\"Type in the name of the file to test: \")\n",
        "    data = load_data(filename)\n",
        "\n",
        "    print(\"\\nType the number of the algorithm you want to run.\\n\")\n",
        "    print(\"1) Forward Selection\")\n",
        "    print(\"2) Backward Elimination\")\n",
        "\n",
        "    choice = int(input())\n",
        "\n",
        "    num_features = data.shape[1] - 1\n",
        "    num_instances = data.shape[0]\n",
        "    print(f\"\\nThis dataset has {num_features} features (not including the class attribute), with {num_instances} instances.\\n\")\n",
        "\n",
        "    initial_accuracy = leave_one_out_cross_validation(data, list(range(1, num_features + 1)))\n",
        "    print(f\"Running nearest neighbor with all {num_features} features, using \\\"leaving-one-out\\\" evaluation, I get an accuracy of {initial_accuracy:.1f}%\\n\")\n",
        "\n",
        "    if choice == 1:\n",
        "        best_features, best_accuracy, search_path, accuracy_path = forward_selection(data)\n",
        "    elif choice == 2:\n",
        "        best_features, best_accuracy, search_path, accuracy_path = backward_elimination(data)\n",
        "    else:\n",
        "        print(\"Invalid choice. Please select 1 or 2.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}